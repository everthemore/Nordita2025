{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db7abeb7-df2c-4003-b8e7-cafb81d09743",
   "metadata": {},
   "source": [
    "# Policy gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505abac0-4b49-4207-8859-402bd7b9a06f",
   "metadata": {},
   "source": [
    "## But first, Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0042fcbe-5a70-4ccf-98c7-045e118bc8cc",
   "metadata": {},
   "source": [
    "From here on, we will do everything in Gymnasium format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41161d73-0e40-4e25-a621-5c7b69ed49d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A custom Gymnasium environment skeleton.\n",
    "    \"\"\"\n",
    "    metadata = {\"render.modes\": [\"human\"]}  # Optional, define render modes\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define action and observation spaces\n",
    "        # Example: Discrete action space with 2 actions (0, 1)\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        # Example: Continuous observation space (1D array of size 3)\n",
    "        self.observation_space = spaces.Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32)\n",
    "        \n",
    "        # Initialize environment state\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Reset the environment to an initial state.\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Example: Initialize the state\n",
    "        self.state = np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        \n",
    "        # Return the initial observation, and a possible dictionary with information\n",
    "        return self.state, {'message':'successfully reset!', 'hello':'world'}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Perform one step in the environment.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Update the environment state (example logic)\n",
    "        self.state = self.state + action - 0.5  # Dummy dynamics\n",
    "        \n",
    "        # Compute the reward (example logic)\n",
    "        reward = -np.sum(np.square(self.state))  \n",
    "        \n",
    "        # Check if the episode is terminated (example logic) -> reach terminal state of MDP\n",
    "        terminated = np.linalg.norm(self.state) > 10.0  \n",
    "\n",
    "        # Flag if the episode was truncated -> terminated not because reaching MDP endpoint, but e.g. because too many steps taken\n",
    "        truncated = False \n",
    "        \n",
    "        # Additional info (can be empty)\n",
    "        info = {}\n",
    "        \n",
    "        return self.state, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"\n",
    "        Render the environment (optional).\n",
    "        \"\"\"\n",
    "        print(f\"State: {self.state}\")\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Clean up resources (optional).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "#Example usage:\n",
    "env = CustomEnv()\n",
    "obs, info = env.reset()\n",
    "print(\"Initial Observation:\", obs)\n",
    "action = env.action_space.sample()\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "done = terminated or truncated\n",
    "print(\"Step:\", obs, reward, done, info)\n",
    "\n",
    "while not done:\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    print(\"Step:\", obs, reward, done, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeab3a3-e734-4031-af74-5a52a28e050c",
   "metadata": {},
   "source": [
    "## Now on to REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5488269-b143-4cee-a25a-7124c2a3f0cb",
   "metadata": {},
   "source": [
    "### Another standard benchmark: the cartpole problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8bfdf3-69f8-44de-bce8-bc80190e5de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Set up the CartPole environment\n",
    "env = gym.make(\"CartPole-v1\")#, render_mode=\"human\")#rgb_array\")\n",
    "\n",
    "# Check state and action space\n",
    "state_dim = ...\n",
    "action_dim = ...\n",
    "\n",
    "print(f\"State dimension: {state_dim}, Action dimension: {action_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa5cb58-d2a3-4c41-866b-14799c30d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim),\n",
    "            nn.Softmax(dim=-1)  # Output probabilities for actions\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.fc(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e1842-079c-49e7-be23-9c1a74b26304",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def select_action(policy, state):\n",
    "    state = torch.tensor(state, dtype=torch.float32)  # Convert state to tensor\n",
    "\n",
    "    # Predict action probabilities using policy network\n",
    "    action_probs = ...\n",
    "\n",
    "    # Here is a trick: use the Categorial distribution so we can use '.sample()'\n",
    "    # Without it, i.e. if we just use the softmax, we would use np.random.choice perhaps\n",
    "    # Similarly, Categorical provides the .log_prob(), which we would have to do separately otherwise\n",
    "    # (And finally, Categorical works nicely if we wanted to use batches)\n",
    "    \n",
    "    action_dist = torch.distributions.Categorical(action_probs)  # Categorical distribution\n",
    "    action = action_dist.sample()                    # Sample an action\n",
    "    return action.item(), action_dist.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf4ea1f-2fe9-4c05-be98-f628ba99a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize policy network\n",
    "policy = PolicyNetwork(state_dim, action_dim)\n",
    "\n",
    "# Test action sampling\n",
    "state, info = env.reset()\n",
    "action, log_prob = select_action(policy, state)\n",
    "print(f\"Sampled action: {action}, Log-probability: {log_prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5059c96b-2b9f-4922-b14f-0c358ef42ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectory(policy, env, max_steps=200):\n",
    "    states, actions, log_probs, rewards = [], [], [], []\n",
    "    state, info = env.reset()\n",
    "\n",
    "    # The trajectory can be max_steps long\n",
    "    for _ in range(max_steps):\n",
    "\n",
    "        # For EACH step, we want to store the action and log( pi(a|s) )\n",
    "        action, log_prob = select_action(policy, state)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Store data\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        # Prepare state for next iteration of for loop\n",
    "        state = next_state\n",
    "    \n",
    "    return states, actions, log_probs, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10662b9-52e4-4d8f-8d49-9ac4964e466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma=0.99):\n",
    "    returns = []\n",
    "\n",
    "    # Should return G_0, G_1, G_2, ... G_t\n",
    "    G = 0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)  # Insert at the beginning\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a31071-e536-4e8c-a952-85573989b85f",
   "metadata": {},
   "source": [
    "## The REINFORCE update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ad8de3-55dc-48ca-8910-257b74cc21a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
    "def reinforce_update(policy, optimizer, log_probs, returns):\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "    # It is a good idea to normalize the returns, but we don't HAVE to\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 1e-8) \n",
    "    \n",
    "    loss = 0\n",
    "    # The gradient is summed over all the steps in a trajectory (hence for loop), and each term has G_t * log_prop(a_t|s_t)\n",
    "    for log_prob, G in zip(log_probs, returns):\n",
    "        loss -= log_prob * G  # Policy gradient loss (negative because we minimize)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a24e203-2ebd-42ea-9b1b-d6127b81a366",
   "metadata": {},
   "source": [
    "## Let's train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f505659-da75-4e50-a8ad-26097fec1997",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "gamma = 0.99\n",
    "\n",
    "reward_progression = []\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    # Collect a single trajectory\n",
    "    states, actions, log_probs, rewards = collect_trajectory(policy, env)\n",
    "    \n",
    "    # Compute the returns for the rewards collected\n",
    "    returns = compute_returns(rewards, gamma)\n",
    "    \n",
    "    # Update policy\n",
    "    reinforce_update(policy, optimizer, log_probs, returns)\n",
    "    \n",
    "    # Logging\n",
    "    total_reward = sum(rewards)\n",
    "    reward_progression.append(total_reward)\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa919c3-f0aa-42e1-8056-964cae99b7d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
