{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db7abeb7-df2c-4003-b8e7-cafb81d09743",
   "metadata": {},
   "source": [
    "# Deep Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0042fcbe-5a70-4ccf-98c7-045e118bc8cc",
   "metadata": {},
   "source": [
    "Q-learning is great, because we have a convergence / optimality statement (Bellman equation). But severely limited to small environments, because storing the Q-table in memory is prohibitive. Also not very well suited for continuous spaces, but we'll get to that later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32970b1a-1148-4ac0-8afa-65e6852b4a24",
   "metadata": {},
   "source": [
    "We'll still use the same sort of environment for now, slightly different implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f8bfdf3-69f8-44de-bce8-bc80190e5de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self, size=(3, 3), goal=(2, 2)):\n",
    "        self.size = size\n",
    "        self.goal = goal\n",
    "        self.state = (0, 0)  # Start at the top-left corner\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to the initial state.\"\"\"\n",
    "        self.state = (0, 0)\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action and return the next state, reward, and if done.\"\"\"\n",
    "        if action == 'up':\n",
    "            self.state = (max(0, self.state[0] - 1), self.state[1])\n",
    "        elif action == 'down':\n",
    "            self.state = (min(self.size[0] - 1, self.state[0] + 1), self.state[1])\n",
    "        elif action == 'left':\n",
    "            self.state = (self.state[0], max(0, self.state[1] - 1))\n",
    "        elif action == 'right':\n",
    "            self.state = (self.state[0], min(self.size[1] - 1, self.state[1] + 1))\n",
    "\n",
    "        # Reward of -1 for each step, +10 for reaching the goal\n",
    "        reward = 10 if self.state == self.goal else -1\n",
    "        done = self.state == self.goal\n",
    "        return self.state, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Visualize the gridworld.\"\"\"\n",
    "        grid = np.full(self.size, '.')\n",
    "        grid[self.goal] = 'G'  # Goal location\n",
    "        grid[self.state] = 'A'  # Agent location\n",
    "        print('\\n'.join([' '.join(row) for row in grid]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aa5cb58-d2a3-4c41-866b-14799c30d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, gamma=0.9, epsilon=0.1, learning_rate=0.01):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Define the Q-network (simple feed-forward neural network)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, action_dim)\n",
    "        )\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        # Define possible actions\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"\"\"Select an action using epsilon-greedy policy.\"\"\"\n",
    "        # TODO\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Update the Q-network based on the observed transition.\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "\n",
    "        # Get Q-values for current and next state\n",
    "        # ... TODO\n",
    "        q_values = ...\n",
    "        next_q_values = ...\n",
    "\n",
    "        # Compute the target Q-value (TD target) (if done, we don't need the next_q)\n",
    "        target = reward + (1 - done) * self.gamma * torch.max(next_q_values)\n",
    "        \n",
    "        # Update the Q-value for the taken action (i.e. set what they should be)\n",
    "        q_values[0][action] = target\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = self.loss_fn(q_values, self.model(state_tensor))\n",
    "        \n",
    "        # Backpropagate and optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76de09c-beb3-47db-9abb-4766bf50c7a5",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b5e1842-079c-49e7-be23-9c1a74b26304",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: -2\n",
      "Episode 2/100, Total Reward: -226\n",
      "Episode 3/100, Total Reward: -37\n",
      "Episode 4/100, Total Reward: -37\n",
      "Episode 5/100, Total Reward: 7\n",
      "Episode 6/100, Total Reward: 7\n",
      "Episode 7/100, Total Reward: 7\n",
      "Episode 8/100, Total Reward: 5\n",
      "Episode 9/100, Total Reward: 7\n",
      "Episode 10/100, Total Reward: 5\n",
      "Episode 11/100, Total Reward: 7\n",
      "Episode 12/100, Total Reward: 7\n",
      "Episode 13/100, Total Reward: 7\n",
      "Episode 14/100, Total Reward: 7\n",
      "Episode 15/100, Total Reward: -5\n",
      "Episode 16/100, Total Reward: -5\n",
      "Episode 17/100, Total Reward: 7\n",
      "Episode 18/100, Total Reward: 5\n",
      "Episode 19/100, Total Reward: 7\n",
      "Episode 20/100, Total Reward: 7\n",
      "Episode 21/100, Total Reward: 6\n",
      "Episode 22/100, Total Reward: 7\n",
      "Episode 23/100, Total Reward: 6\n",
      "Episode 24/100, Total Reward: 7\n",
      "Episode 25/100, Total Reward: 7\n",
      "Episode 26/100, Total Reward: 6\n",
      "Episode 27/100, Total Reward: 7\n",
      "Episode 28/100, Total Reward: 7\n",
      "Episode 29/100, Total Reward: 6\n",
      "Episode 30/100, Total Reward: 5\n",
      "Episode 31/100, Total Reward: 7\n",
      "Episode 32/100, Total Reward: 7\n",
      "Episode 33/100, Total Reward: 7\n",
      "Episode 34/100, Total Reward: 7\n",
      "Episode 35/100, Total Reward: 7\n",
      "Episode 36/100, Total Reward: 6\n",
      "Episode 37/100, Total Reward: 7\n",
      "Episode 38/100, Total Reward: 7\n",
      "Episode 39/100, Total Reward: 5\n",
      "Episode 40/100, Total Reward: -9\n",
      "Episode 41/100, Total Reward: 7\n",
      "Episode 42/100, Total Reward: 7\n",
      "Episode 43/100, Total Reward: 7\n",
      "Episode 44/100, Total Reward: 7\n",
      "Episode 45/100, Total Reward: 7\n",
      "Episode 46/100, Total Reward: 7\n",
      "Episode 47/100, Total Reward: 7\n",
      "Episode 48/100, Total Reward: 6\n",
      "Episode 49/100, Total Reward: 5\n",
      "Episode 50/100, Total Reward: 7\n",
      "Episode 51/100, Total Reward: 7\n",
      "Episode 52/100, Total Reward: 7\n",
      "Episode 53/100, Total Reward: 7\n",
      "Episode 54/100, Total Reward: 7\n",
      "Episode 55/100, Total Reward: 7\n",
      "Episode 56/100, Total Reward: 7\n",
      "Episode 57/100, Total Reward: 7\n",
      "Episode 58/100, Total Reward: 7\n",
      "Episode 59/100, Total Reward: 6\n",
      "Episode 60/100, Total Reward: 3\n",
      "Episode 61/100, Total Reward: 7\n",
      "Episode 62/100, Total Reward: 7\n",
      "Episode 63/100, Total Reward: 7\n",
      "Episode 64/100, Total Reward: 7\n",
      "Episode 65/100, Total Reward: 7\n",
      "Episode 66/100, Total Reward: 7\n",
      "Episode 67/100, Total Reward: 7\n",
      "Episode 68/100, Total Reward: 7\n",
      "Episode 69/100, Total Reward: 7\n",
      "Episode 70/100, Total Reward: 7\n",
      "Episode 71/100, Total Reward: 6\n",
      "Episode 72/100, Total Reward: 7\n",
      "Episode 73/100, Total Reward: 7\n",
      "Episode 74/100, Total Reward: 7\n",
      "Episode 75/100, Total Reward: 7\n",
      "Episode 76/100, Total Reward: 6\n",
      "Episode 77/100, Total Reward: 7\n",
      "Episode 78/100, Total Reward: 7\n",
      "Episode 79/100, Total Reward: 5\n",
      "Episode 80/100, Total Reward: 5\n",
      "Episode 81/100, Total Reward: 5\n",
      "Episode 82/100, Total Reward: 6\n",
      "Episode 83/100, Total Reward: 7\n",
      "Episode 84/100, Total Reward: 7\n",
      "Episode 85/100, Total Reward: 7\n",
      "Episode 86/100, Total Reward: 7\n",
      "Episode 87/100, Total Reward: 5\n",
      "Episode 88/100, Total Reward: 7\n",
      "Episode 89/100, Total Reward: 7\n",
      "Episode 90/100, Total Reward: 7\n",
      "Episode 91/100, Total Reward: 7\n",
      "Episode 92/100, Total Reward: 6\n",
      "Episode 93/100, Total Reward: 7\n",
      "Episode 94/100, Total Reward: 7\n",
      "Episode 95/100, Total Reward: 7\n",
      "Episode 96/100, Total Reward: 7\n",
      "Episode 97/100, Total Reward: 7\n",
      "Episode 98/100, Total Reward: 7\n",
      "Episode 99/100, Total Reward: 7\n",
      "Episode 100/100, Total Reward: 7\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld()\n",
    "agent = DQNAgent(state_dim=2, action_dim=4)\n",
    "\n",
    "num_episodes = 100\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Render the environment (optional)\n",
    "        #env.render()\n",
    "        \n",
    "        # Choose an action\n",
    "        action = # ... \n",
    "        \n",
    "        # Take the action in the environment\n",
    "        next_state, reward, done = env.step( agent.actions[action] )\n",
    "        \n",
    "        # Learn from the experience\n",
    "        agent.learn(state, action, reward, next_state, done)\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    \n",
    "    print(f\"Episode {episode+1}/{num_episodes}, Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5059c96b-2b9f-4922-b14f-0c358ef42ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
