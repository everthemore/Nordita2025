{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cd85b2-2847-4adc-aa57-b74fe8f3699e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Training the PPO agent...\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92.7     |\n",
      "|    ep_rew_mean     | -171     |\n",
      "| time/              |          |\n",
      "|    fps             | 8013     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 96.9         |\n",
      "|    ep_rew_mean          | -176         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5214         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069124317 |\n",
      "|    clip_fraction        | 0.0383       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.00332     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 409          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00368     |\n",
      "|    value_loss           | 1.05e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | -147        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4683        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008740682 |\n",
      "|    clip_fraction        | 0.049       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.0377      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 227         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00543    |\n",
      "|    value_loss           | 795         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 105        |\n",
      "|    ep_rew_mean          | -137       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4445       |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 32768      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00863683 |\n",
      "|    clip_fraction        | 0.0957     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.35      |\n",
      "|    explained_variance   | 0.177      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 118        |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0104    |\n",
      "|    value_loss           | 349        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 116         |\n",
      "|    ep_rew_mean          | -123        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4130        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012388585 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.33        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 84.8        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    value_loss           | 275         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 134         |\n",
      "|    ep_rew_mean          | -95.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3957        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012457117 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.357       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 85.2        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 260         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 149         |\n",
      "|    ep_rew_mean          | -82         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3680        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011905516 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.375       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 165         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    value_loss           | 186         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 179         |\n",
      "|    ep_rew_mean          | -78.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3494        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011471582 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.46        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 46.5        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00975    |\n",
      "|    value_loss           | 177         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -77.1        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3397         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077878167 |\n",
      "|    clip_fraction        | 0.0552       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.15        |\n",
      "|    explained_variance   | 0.582        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 91.3         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00666     |\n",
      "|    value_loss           | 183          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 230          |\n",
      "|    ep_rew_mean          | -73.3        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3109         |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073680473 |\n",
      "|    clip_fraction        | 0.0609       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.643        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 75.1         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00628     |\n",
      "|    value_loss           | 159          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 285         |\n",
      "|    ep_rew_mean          | -64.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2776        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008897794 |\n",
      "|    clip_fraction        | 0.0923      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.637       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 60.3        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00754    |\n",
      "|    value_loss           | 126         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 317          |\n",
      "|    ep_rew_mean          | -48.1        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2615         |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 37           |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069615464 |\n",
      "|    clip_fraction        | 0.0635       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | 0.637        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 38.7         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00562     |\n",
      "|    value_loss           | 110          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 372          |\n",
      "|    ep_rew_mean          | -39.2        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2383         |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 44           |\n",
      "|    total_timesteps      | 106496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078057433 |\n",
      "|    clip_fraction        | 0.0899       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.726        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27.7         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00762     |\n",
      "|    value_loss           | 71.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 424         |\n",
      "|    ep_rew_mean          | -19         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2248        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 51          |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008922841 |\n",
      "|    clip_fraction        | 0.0911      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.8         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.3        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00925    |\n",
      "|    value_loss           | 47.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 478         |\n",
      "|    ep_rew_mean          | -5.69       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2094        |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006974045 |\n",
      "|    clip_fraction        | 0.0704      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.716       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.97        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00609    |\n",
      "|    value_loss           | 81.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 531          |\n",
      "|    ep_rew_mean          | 6            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1972         |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 66           |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0090035405 |\n",
      "|    clip_fraction        | 0.0734       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.771        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.56         |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00775     |\n",
      "|    value_loss           | 38.8         |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Step 1: Create the LunarLander-v2 environment\n",
    "env_name = \"LunarLander-v3\"\n",
    "env = make_vec_env(env_name, n_envs=4)  # Vectorized environment for parallelization\n",
    "\n",
    "# Step 2: Define the PPO model\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",       # Multi-layer perceptron policy\n",
    "    env,               # Environment\n",
    "    verbose=1,         # Print training information\n",
    "    learning_rate=3e-4,  # Learning rate for the optimizer\n",
    "    n_steps=2048,      # Number of steps to run per environment per update\n",
    "    batch_size=64,     # Minibatch size for gradient updates\n",
    "    n_epochs=10,       # Number of epochs to optimize the surrogate loss\n",
    "    gamma=0.99,        # Discount factor\n",
    "    gae_lambda=0.95,   # GAE (Generalized Advantage Estimation) lambda\n",
    "    clip_range=0.2,    # Clipping parameter for PPO\n",
    ")\n",
    "\n",
    "# Step 3: Train the PPO agent\n",
    "timesteps = 200000  # Total training steps\n",
    "print(\"Training the PPO agent...\")\n",
    "model.learn(total_timesteps=timesteps)\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Step 4: Evaluate the agent\n",
    "def evaluate_agent(model, env, n_episodes=5):\n",
    "    \"\"\"\n",
    "    Evaluate the trained PPO agent in the given environment.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    for episode in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)  # Use the trained policy\n",
    "            obs, reward, done, _, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "        total_rewards.append(episode_reward)\n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward:.2f}\")\n",
    "    avg_reward = sum(total_rewards) / n_episodes\n",
    "    print(f\"Average reward over {n_episodes} episodes: {avg_reward:.2f}\")\n",
    "\n",
    "# Evaluate the trained model\n",
    "print(\"Evaluating the PPO agent...\")\n",
    "evaluate_agent(model, gym.make(env_name))\n",
    "\n",
    "# Step 5: Save the model\n",
    "model.save(\"ppo_lunarlander\")\n",
    "print(\"Model saved as 'ppo_lunarlander.zip'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd29bf23-a384-45e6-a9aa-ef3c3c30fc98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load the model and continue training (optional)\n",
    "# model = PPO.load(\"ppo_lunarlander\", env=env)\n",
    "# model.learn(total_timesteps=100000)\n",
    "# model.save(\"ppo_lunarlander\")\n",
    "# print(\"Model saved as 'ppo_lunarlander.zip'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d9363e7-7f28-4a27-97cc-88e9c7982dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")  # Enable rendering\n",
    "test_model = PPO.load(\"ppo_lunarlander\")\n",
    "\n",
    "# Step 2: Evaluate the agent\n",
    "obs,_ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    # Step 3: Predict action using the trained model\n",
    "    action, _ = test_model.predict(obs, deterministic=True)  # Use deterministic policy for evaluation\n",
    "    obs, reward, done, _, info = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "# Step 4: Close the environment after evaluation\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7e8f17-c5c4-446e-bfa9-df6d26b7c879",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
