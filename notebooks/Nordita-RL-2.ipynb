{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "391878b3-afa8-4375-ac1f-48036f387a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dca33a75-575f-47c2-8630-f162e1158445",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BananaGridWorld:\n",
    "    def __init__(self):\n",
    "        self.grid_size = 3\n",
    "        self.start_position = (2, 0)  # Bottom-left corner\n",
    "        self.goal_position = (0, 2)  # Top-right corner (Banana)\n",
    "        self.agent_position = self.start_position\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Reset the environment to the initial state. \"\"\"\n",
    "        self.agent_position = self.start_position\n",
    "        return self.agent_position\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take a step in the environment.\n",
    "\n",
    "        Args:\n",
    "            action (str): ['up', 'down', 'left', 'right']\n",
    "\n",
    "        Returns:\n",
    "            tuple: (new_position, reward, done)\n",
    "        \"\"\"\n",
    "\n",
    "        # Decompose into x and y\n",
    "        x, y = self.agent_position\n",
    "\n",
    "        # This can be confusing (x left/right vs y up/down, but I did this only because of\n",
    "        # drawing the state in the render() function below. You can change this to the more\n",
    "        # intuitive version and then have a better render() function\n",
    "        if action == 'up' and x > 0:\n",
    "            x -= 1\n",
    "        elif action == 'down' and x < self.grid_size - 1:\n",
    "            x += 1\n",
    "        elif action == 'left' and y > 0:\n",
    "            y -= 1\n",
    "        elif action == 'right' and y < self.grid_size - 1:\n",
    "            y += 1\n",
    "\n",
    "        # Update the agent's position\n",
    "        self.agent_position = (x, y)\n",
    "\n",
    "        # Return \n",
    "        # ... TODO\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Print the grid with the agent's current position.\"\"\"\n",
    "        # Make a grid_size x grid_size array filled with '-'\n",
    "        grid = [['-' for _ in range(self.grid_size)] for _ in range(self.grid_size)]\n",
    "\n",
    "        # Replace the - at the monkey's position with an M\n",
    "        x, y = self.agent_position\n",
    "        grid[x][y] = 'M'  # M for Monkeuy\n",
    "\n",
    "        # And same for the Banana\n",
    "        x, y = self.goal_position\n",
    "        grid[x][y] = 'B'\n",
    "\n",
    "        # Print the grid\n",
    "        for row in grid:\n",
    "            print(' '.join(row))\n",
    "        print() # With an extra newline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9b6197-fa27-42fb-a178-a9e34d05c05e",
   "metadata": {},
   "source": [
    "## Testing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6eeb358e-a34b-41d5-b2c6-dcfd21779636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - B\n",
      "- - -\n",
      "M - -\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m testenv\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      3\u001b[0m testenv\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m----> 5\u001b[0m _, r, done \u001b[38;5;241m=\u001b[39m testenv\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m testenv\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe got reward \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and the episode is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mdone\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m done\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "testenv = BananaGridWorld()\n",
    "testenv.reset()\n",
    "testenv.render()\n",
    "\n",
    "_, r, done = testenv.step('right')\n",
    "testenv.render()\n",
    "print(f\"We got reward {r} and the episode is {'' if done else 'not'} done\")\n",
    "\n",
    "_, r, done = testenv.step('right')\n",
    "testenv.render()\n",
    "_, r, done = testenv.step('up')\n",
    "testenv.render()\n",
    "_, r, done = testenv.step('up')\n",
    "testenv.render()\n",
    "\n",
    "testenv.render()\n",
    "print(f\"We got reward {r} and the episode is {'' if done else 'not'} done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d9a750-6286-4981-84e2-7d03aa20f811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7487eda3-f3c4-48eb-88dc-150716aa5b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "- - B\n",
      "- - -\n",
      "M - -\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     42\u001b[0m     action \u001b[38;5;241m=\u001b[39m choose_action(observation)\n\u001b[0;32m---> 43\u001b[0m     next_observation, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     44\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# Update Q-value\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# Actions the agent can take\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "# Initialize the environment\n",
    "env = BananaGridWorld()\n",
    "\n",
    "# Q-learning parameters\n",
    "q_table = {}  # Q-table, which we will fill with *tuples* of the form ( state, action )\n",
    "alpha = 0.1  # Learning rate\n",
    "epsilon = 0.1\n",
    "\n",
    "def get_q_value(state, action):\n",
    "    # Implement returning the value at key '(state,action)', and include a default of 0 if (state,action) is not in the dictionary\n",
    "    # using dict.get( key, default_value )\n",
    "    return 0\n",
    "\n",
    "def update_q_value(state, action, reward):\n",
    "    # Implement the update rule to update the value at q_table[(state,action)] \n",
    "    current_q = get_q_value(state, action)\n",
    "    #q_table[(state, action)] = ...\n",
    "\n",
    "def choose_action(state):\n",
    "    # Implement an epsilon-greedy policy (so w/ prob epsilon choose randomly from actions, otherwise pick according to Q\n",
    "    if np.random.rand() < epsilon:\n",
    "        # Explore, pick randomly\n",
    "        return 0\n",
    "    else:\n",
    "        # Exploit, pick action with highest q_value\n",
    "        return 0\n",
    "\n",
    "# Run Q-learning agent\n",
    "num_episodes = 100\n",
    "for episode in range(num_episodes):\n",
    "\n",
    "    # Restart at each episode\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    print(f\"Episode {episode + 1}\")\n",
    "    env.render()\n",
    "\n",
    "    while not done:\n",
    "        action = choose_action(observation)\n",
    "        next_observation, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        # Update Q-value\n",
    "        update_q_value(observation, action, reward, next_observation)\n",
    "        observation = next_observation\n",
    "\n",
    "        env.render()\n",
    "\n",
    "    print(f\"Episode finished with total reward: {total_reward}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4d16c0-b031-4ede-94c1-804a80529dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c65a39be-2016-42d7-9ed5-2cd341958d9c",
   "metadata": {},
   "source": [
    "## Todo's:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef08ebfe-215a-4140-93a0-a7a127322f66",
   "metadata": {},
   "source": [
    "* 1) Make plots of the total reward over time (episodes)\n",
    "  2) See how changing the hyperparameters influences this (e.g. gamma = 0 should do what?)\n",
    "  3) Compare SARSA and Q-learning in the case with cliffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e987fc-2c0b-43df-becd-be98e442963b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
