{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db7abeb7-df2c-4003-b8e7-cafb81d09743",
   "metadata": {},
   "source": [
    "# Actor-Critic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f8bfdf3-69f8-44de-bce8-bc80190e5de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Actor network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        probs = F.softmax(self.fc2(x), dim=-1)  # Action probabilities\n",
    "        return probs\n",
    "\n",
    "# Instantiate the policy network\n",
    "state_dim = 4   # Example: state has 4 features\n",
    "action_dim = 2  # Example: 2 possible actions\n",
    "policy_net = PolicyNetwork(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f4350f9-69c3-4957-84d1-e9980b43ec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic network\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        value = self.fc2(x)  # Scalar value output\n",
    "        return value\n",
    "\n",
    "# Instantiate the value network\n",
    "value_net = ValueNetwork(state_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7b054d-4376-4493-9dbd-a3a14d981e15",
   "metadata": {},
   "source": [
    "The advantage is derived from the TD error, which can be computed as:\n",
    "$$ \\delta_t = R_{t+1} + \\gamma V(s_{t+1}) - V(s_t)  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5554d252-2251-49f5-b480-c2ca6956e643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TD Error: 1.0241752862930298\n"
     ]
    }
   ],
   "source": [
    "# Compute TD error and advantage\n",
    "def compute_td_error(state, reward, next_state, done, gamma=0.99):\n",
    "    with torch.no_grad():\n",
    "        td_target = reward + gamma * value_net(next_state) * (1 - done)  # Handle terminal states\n",
    "        td_error = td_target - value_net(state) # target - critic prediction\n",
    "    return td_error\n",
    "\n",
    "def compute_td_error(state, reward, next_state, done, gamma=0.99):\n",
    "    value = value_net(state).squeeze()           # V(s)\n",
    "    next_value = value_net(next_state).squeeze() # V(s')\n",
    "    next_value = next_value * (1 - done)         # Mask terminal states (done is 1 for terminal states)\n",
    "    \n",
    "    # TD Error (delta_t)\n",
    "    td_error = reward + gamma * next_value - value\n",
    "    return td_error\n",
    "\n",
    "# Example usage\n",
    "state = torch.tensor([1.0, 0.5, -0.2, 0.3])      # Example current state\n",
    "next_state = torch.tensor([1.1, 0.4, -0.1, 0.3])  # Example next state\n",
    "reward = torch.tensor(1.0)                        # Example reward\n",
    "done = torch.tensor(0.0)                          # Example \"not done\"\n",
    "\n",
    "td_error = compute_td_error(state, reward, next_state, done)\n",
    "print(\"TD Error:\", td_error.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768e1dcb-d524-42ba-84be-f7db3dc81116",
   "metadata": {},
   "source": [
    "## Computing the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f863351-4601-41b2-be22-4dab65df9424",
   "metadata": {},
   "source": [
    "The actor uses the advantage $A(s_t,a_t)$ to adjust the policy, while the critic minimizes the TD error.\n",
    "\n",
    "*Actor loss*:\n",
    "$$ L_{\\textrm{actor}} = -\\log \\pi_\\theta(a_t|s_t)A(s_t, a_t) $$\n",
    "\n",
    "*Critic loss*:\n",
    "$$ L_{\\textrm{critic}} = \\delta_t^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2e04746-1981-416d-93e1-fcd3edc6cdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "policy_optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "value_optimizer = optim.Adam(value_net.parameters(), lr=1e-3)\n",
    "\n",
    "# Compute losses\n",
    "def compute_losses(state, action, reward, next_state, done):\n",
    "    \n",
    "    # TD Error (delta_t)\n",
    "    td_error = compute_td_error(state, reward, next_state, done)\n",
    "\n",
    "    # Critic loss (mean squared error)\n",
    "    critic_loss = td_error.pow(2).mean()\n",
    "\n",
    "    # Compute log-probability of the action\n",
    "    probs = policy_net(state)\n",
    "    dist = torch.distributions.Categorical(probs)\n",
    "    log_prob = dist.log_prob(action)\n",
    "\n",
    "    # Actor loss (policy gradient with advantage)\n",
    "    actor_loss = -(log_prob * td_error.detach())  # Detach TD error to avoid backprop through critic\n",
    "    return actor_loss, critic_loss\n",
    "\n",
    "# Example usage\n",
    "action = torch.tensor(1)  # Example action taken (discrete action space)\n",
    "actor_loss, critic_loss = compute_losses(state, action, reward, next_state, done)\n",
    "\n",
    "# Optimize policy network\n",
    "policy_optimizer.zero_grad()\n",
    "actor_loss.backward()\n",
    "policy_optimizer.step()\n",
    "\n",
    "# Optimize value network\n",
    "value_optimizer.zero_grad()\n",
    "critic_loss.backward()\n",
    "value_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "440c688f-00c8-4924-9d73-c0eff98d25f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running episode 0...\n",
      "\tFinished with reward 61.0\n",
      "Running episode 1...\n",
      "\tFinished with reward 49.0\n",
      "Running episode 2...\n",
      "\tFinished with reward 61.0\n",
      "Running episode 3...\n",
      "\tFinished with reward 46.0\n",
      "Running episode 4...\n",
      "\tFinished with reward 71.0\n",
      "Running episode 5...\n",
      "\tFinished with reward 32.0\n",
      "Running episode 6...\n",
      "\tFinished with reward 91.0\n",
      "Running episode 7...\n",
      "\tFinished with reward 69.0\n",
      "Running episode 8...\n",
      "\tFinished with reward 68.0\n",
      "Running episode 9...\n",
      "\tFinished with reward 47.0\n",
      "Running episode 10...\n",
      "\tFinished with reward 35.0\n",
      "Running episode 11...\n",
      "\tFinished with reward 69.0\n",
      "Running episode 12...\n",
      "\tFinished with reward 33.0\n",
      "Running episode 13...\n",
      "\tFinished with reward 33.0\n",
      "Running episode 14...\n",
      "\tFinished with reward 34.0\n",
      "Running episode 15...\n",
      "\tFinished with reward 36.0\n",
      "Running episode 16...\n",
      "\tFinished with reward 41.0\n",
      "Running episode 17...\n",
      "\tFinished with reward 48.0\n",
      "Running episode 18...\n",
      "\tFinished with reward 32.0\n",
      "Running episode 19...\n",
      "\tFinished with reward 50.0\n",
      "Running episode 20...\n",
      "\tFinished with reward 50.0\n",
      "Running episode 21...\n",
      "\tFinished with reward 48.0\n",
      "Running episode 22...\n",
      "\tFinished with reward 34.0\n",
      "Running episode 23...\n",
      "\tFinished with reward 37.0\n",
      "Running episode 24...\n",
      "\tFinished with reward 43.0\n",
      "Running episode 25...\n",
      "\tFinished with reward 52.0\n",
      "Running episode 26...\n",
      "\tFinished with reward 39.0\n",
      "Running episode 27...\n",
      "\tFinished with reward 41.0\n",
      "Running episode 28...\n",
      "\tFinished with reward 57.0\n",
      "Running episode 29...\n",
      "\tFinished with reward 66.0\n",
      "Running episode 30...\n",
      "\tFinished with reward 41.0\n",
      "Running episode 31...\n",
      "\tFinished with reward 45.0\n",
      "Running episode 32...\n",
      "\tFinished with reward 91.0\n",
      "Running episode 33...\n",
      "\tFinished with reward 61.0\n",
      "Running episode 34...\n",
      "\tFinished with reward 53.0\n",
      "Running episode 35...\n",
      "\tFinished with reward 77.0\n",
      "Running episode 36...\n",
      "\tFinished with reward 58.0\n",
      "Running episode 37...\n",
      "\tFinished with reward 52.0\n",
      "Running episode 38...\n",
      "\tFinished with reward 53.0\n",
      "Running episode 39...\n",
      "\tFinished with reward 77.0\n",
      "Running episode 40...\n",
      "\tFinished with reward 45.0\n",
      "Running episode 41...\n",
      "\tFinished with reward 108.0\n",
      "Running episode 42...\n",
      "\tFinished with reward 63.0\n",
      "Running episode 43...\n",
      "\tFinished with reward 48.0\n",
      "Running episode 44...\n",
      "\tFinished with reward 49.0\n",
      "Running episode 45...\n",
      "\tFinished with reward 57.0\n",
      "Running episode 46...\n",
      "\tFinished with reward 87.0\n",
      "Running episode 47...\n",
      "\tFinished with reward 56.0\n",
      "Running episode 48...\n",
      "\tFinished with reward 57.0\n",
      "Running episode 49...\n",
      "\tFinished with reward 65.0\n",
      "Running episode 50...\n",
      "\tFinished with reward 85.0\n",
      "Running episode 51...\n",
      "\tFinished with reward 65.0\n",
      "Running episode 52...\n",
      "\tFinished with reward 60.0\n",
      "Running episode 53...\n",
      "\tFinished with reward 133.0\n",
      "Running episode 54...\n",
      "\tFinished with reward 72.0\n",
      "Running episode 55...\n",
      "\tFinished with reward 43.0\n",
      "Running episode 56...\n",
      "\tFinished with reward 59.0\n",
      "Running episode 57...\n",
      "\tFinished with reward 90.0\n",
      "Running episode 58...\n",
      "\tFinished with reward 63.0\n",
      "Running episode 59...\n",
      "\tFinished with reward 65.0\n",
      "Running episode 60...\n",
      "\tFinished with reward 90.0\n",
      "Running episode 61...\n",
      "\tFinished with reward 75.0\n",
      "Running episode 62...\n",
      "\tFinished with reward 58.0\n",
      "Running episode 63...\n",
      "\tFinished with reward 40.0\n",
      "Running episode 64...\n",
      "\tFinished with reward 31.0\n",
      "Running episode 65...\n",
      "\tFinished with reward 45.0\n",
      "Running episode 66...\n",
      "\tFinished with reward 85.0\n",
      "Running episode 67...\n",
      "\tFinished with reward 53.0\n",
      "Running episode 68...\n",
      "\tFinished with reward 73.0\n",
      "Running episode 69...\n",
      "\tFinished with reward 39.0\n",
      "Running episode 70...\n",
      "\tFinished with reward 96.0\n",
      "Running episode 71...\n",
      "\tFinished with reward 41.0\n",
      "Running episode 72...\n",
      "\tFinished with reward 75.0\n",
      "Running episode 73...\n",
      "\tFinished with reward 49.0\n",
      "Running episode 74...\n",
      "\tFinished with reward 43.0\n",
      "Running episode 75...\n",
      "\tFinished with reward 53.0\n",
      "Running episode 76...\n",
      "\tFinished with reward 94.0\n",
      "Running episode 77...\n",
      "\tFinished with reward 58.0\n",
      "Running episode 78...\n",
      "\tFinished with reward 54.0\n",
      "Running episode 79...\n",
      "\tFinished with reward 75.0\n",
      "Running episode 80...\n",
      "\tFinished with reward 61.0\n",
      "Running episode 81...\n",
      "\tFinished with reward 59.0\n",
      "Running episode 82...\n",
      "\tFinished with reward 72.0\n",
      "Running episode 83...\n",
      "\tFinished with reward 65.0\n",
      "Running episode 84...\n",
      "\tFinished with reward 131.0\n",
      "Running episode 85...\n",
      "\tFinished with reward 111.0\n",
      "Running episode 86...\n",
      "\tFinished with reward 74.0\n",
      "Running episode 87...\n",
      "\tFinished with reward 71.0\n",
      "Running episode 88...\n",
      "\tFinished with reward 79.0\n",
      "Running episode 89...\n",
      "\tFinished with reward 65.0\n",
      "Running episode 90...\n",
      "\tFinished with reward 159.0\n",
      "Running episode 91...\n",
      "\tFinished with reward 36.0\n",
      "Running episode 92...\n",
      "\tFinished with reward 67.0\n",
      "Running episode 93...\n",
      "\tFinished with reward 56.0\n",
      "Running episode 94...\n",
      "\tFinished with reward 33.0\n",
      "Running episode 95...\n",
      "\tFinished with reward 48.0\n",
      "Running episode 96...\n",
      "\tFinished with reward 83.0\n",
      "Running episode 97...\n",
      "\tFinished with reward 67.0\n",
      "Running episode 98...\n",
      "\tFinished with reward 55.0\n",
      "Running episode 99...\n",
      "\tFinished with reward 60.0\n",
      "Running episode 100...\n",
      "\tFinished with reward 42.0\n",
      "Running episode 101...\n",
      "\tFinished with reward 39.0\n",
      "Running episode 102...\n",
      "\tFinished with reward 75.0\n",
      "Running episode 103...\n",
      "\tFinished with reward 93.0\n",
      "Running episode 104...\n",
      "\tFinished with reward 81.0\n",
      "Running episode 105...\n",
      "\tFinished with reward 65.0\n",
      "Running episode 106...\n",
      "\tFinished with reward 49.0\n",
      "Running episode 107...\n",
      "\tFinished with reward 67.0\n",
      "Running episode 108...\n",
      "\tFinished with reward 74.0\n",
      "Running episode 109...\n",
      "\tFinished with reward 99.0\n",
      "Running episode 110...\n",
      "\tFinished with reward 51.0\n",
      "Running episode 111...\n",
      "\tFinished with reward 56.0\n",
      "Running episode 112...\n",
      "\tFinished with reward 51.0\n",
      "Running episode 113...\n",
      "\tFinished with reward 63.0\n",
      "Running episode 114...\n",
      "\tFinished with reward 57.0\n",
      "Running episode 115...\n",
      "\tFinished with reward 57.0\n",
      "Running episode 116...\n",
      "\tFinished with reward 64.0\n",
      "Running episode 117...\n",
      "\tFinished with reward 94.0\n",
      "Running episode 118...\n",
      "\tFinished with reward 57.0\n",
      "Running episode 119...\n",
      "\tFinished with reward 110.0\n",
      "Running episode 120...\n",
      "\tFinished with reward 59.0\n",
      "Running episode 121...\n",
      "\tFinished with reward 128.0\n",
      "Running episode 122...\n",
      "\tFinished with reward 51.0\n",
      "Running episode 123...\n",
      "\tFinished with reward 45.0\n",
      "Running episode 124...\n",
      "\tFinished with reward 44.0\n",
      "Running episode 125...\n",
      "\tFinished with reward 51.0\n",
      "Running episode 126...\n",
      "\tFinished with reward 63.0\n",
      "Running episode 127...\n",
      "\tFinished with reward 72.0\n",
      "Running episode 128...\n",
      "\tFinished with reward 58.0\n",
      "Running episode 129...\n",
      "\tFinished with reward 62.0\n",
      "Running episode 130...\n",
      "\tFinished with reward 35.0\n",
      "Running episode 131...\n",
      "\tFinished with reward 78.0\n",
      "Running episode 132...\n",
      "\tFinished with reward 67.0\n",
      "Running episode 133...\n",
      "\tFinished with reward 66.0\n",
      "Running episode 134...\n",
      "\tFinished with reward 48.0\n",
      "Running episode 135...\n",
      "\tFinished with reward 72.0\n",
      "Running episode 136...\n",
      "\tFinished with reward 69.0\n",
      "Running episode 137...\n",
      "\tFinished with reward 43.0\n",
      "Running episode 138...\n",
      "\tFinished with reward 141.0\n",
      "Running episode 139...\n",
      "\tFinished with reward 91.0\n",
      "Running episode 140...\n",
      "\tFinished with reward 59.0\n",
      "Running episode 141...\n",
      "\tFinished with reward 57.0\n",
      "Running episode 142...\n",
      "\tFinished with reward 83.0\n",
      "Running episode 143...\n",
      "\tFinished with reward 145.0\n",
      "Running episode 144...\n",
      "\tFinished with reward 44.0\n",
      "Running episode 145...\n",
      "\tFinished with reward 62.0\n",
      "Running episode 146...\n",
      "\tFinished with reward 58.0\n",
      "Running episode 147...\n",
      "\tFinished with reward 65.0\n",
      "Running episode 148...\n",
      "\tFinished with reward 110.0\n",
      "Running episode 149...\n",
      "\tFinished with reward 51.0\n",
      "Running episode 150...\n",
      "\tFinished with reward 67.0\n",
      "Running episode 151...\n",
      "\tFinished with reward 78.0\n",
      "Running episode 152...\n",
      "\tFinished with reward 94.0\n",
      "Running episode 153...\n",
      "\tFinished with reward 84.0\n",
      "Running episode 154...\n",
      "\tFinished with reward 50.0\n",
      "Running episode 155...\n",
      "\tFinished with reward 65.0\n",
      "Running episode 156...\n",
      "\tFinished with reward 61.0\n",
      "Running episode 157...\n",
      "\tFinished with reward 56.0\n",
      "Running episode 158...\n",
      "\tFinished with reward 133.0\n",
      "Running episode 159...\n",
      "\tFinished with reward 52.0\n",
      "Running episode 160...\n",
      "\tFinished with reward 77.0\n",
      "Running episode 161...\n",
      "\tFinished with reward 143.0\n",
      "Running episode 162...\n",
      "\tFinished with reward 188.0\n",
      "Running episode 163...\n",
      "\tFinished with reward 72.0\n",
      "Running episode 164...\n",
      "\tFinished with reward 70.0\n",
      "Running episode 165...\n",
      "\tFinished with reward 119.0\n",
      "Running episode 166...\n",
      "\tFinished with reward 66.0\n",
      "Running episode 167...\n",
      "\tFinished with reward 69.0\n",
      "Running episode 168...\n",
      "\tFinished with reward 55.0\n",
      "Running episode 169...\n",
      "\tFinished with reward 86.0\n",
      "Running episode 170...\n",
      "\tFinished with reward 72.0\n",
      "Running episode 171...\n",
      "\tFinished with reward 49.0\n",
      "Running episode 172...\n",
      "\tFinished with reward 41.0\n",
      "Running episode 173...\n",
      "\tFinished with reward 63.0\n",
      "Running episode 174...\n",
      "\tFinished with reward 94.0\n",
      "Running episode 175...\n",
      "\tFinished with reward 80.0\n",
      "Running episode 176...\n",
      "\tFinished with reward 193.0\n",
      "Running episode 177...\n",
      "\tFinished with reward 60.0\n",
      "Running episode 178...\n",
      "\tFinished with reward 116.0\n",
      "Running episode 179...\n",
      "\tFinished with reward 113.0\n",
      "Running episode 180...\n",
      "\tFinished with reward 78.0\n",
      "Running episode 181...\n",
      "\tFinished with reward 60.0\n",
      "Running episode 182...\n",
      "\tFinished with reward 65.0\n",
      "Running episode 183...\n",
      "\tFinished with reward 61.0\n",
      "Running episode 184...\n",
      "\tFinished with reward 78.0\n",
      "Running episode 185...\n",
      "\tFinished with reward 72.0\n",
      "Running episode 186...\n",
      "\tFinished with reward 134.0\n",
      "Running episode 187...\n",
      "\tFinished with reward 86.0\n",
      "Running episode 188...\n",
      "\tFinished with reward 166.0\n",
      "Running episode 189...\n",
      "\tFinished with reward 80.0\n",
      "Running episode 190...\n",
      "\tFinished with reward 208.0\n",
      "Running episode 191...\n",
      "\tFinished with reward 272.0\n",
      "Running episode 192...\n",
      "\tFinished with reward 108.0\n",
      "Running episode 193...\n",
      "\tFinished with reward 99.0\n",
      "Running episode 194...\n",
      "\tFinished with reward 142.0\n",
      "Running episode 195...\n",
      "\tFinished with reward 253.0\n",
      "Running episode 196...\n",
      "\tFinished with reward 106.0\n",
      "Running episode 197...\n",
      "\tFinished with reward 231.0\n",
      "Running episode 198...\n",
      "\tFinished with reward 106.0\n",
      "Running episode 199...\n",
      "\tFinished with reward 139.0\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "import gymnasium as gym\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "for episode in range(500):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    print(f\"Running episode {episode}...\")\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "        # Sample action from policy\n",
    "        probs = policy_net(state_tensor)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "\n",
    "        # Step environment\n",
    "        next_state, reward, done, trunc, _ = env.step(action.item())\n",
    "        total_reward += reward\n",
    "\n",
    "        # Convert to tensors\n",
    "        reward_tensor = torch.tensor(reward, dtype=torch.float32)\n",
    "        next_state_tensor = torch.tensor(next_state, dtype=torch.float32)\n",
    "        done_tensor = torch.tensor(float(done), dtype=torch.float32)\n",
    "\n",
    "        # Compute losses\n",
    "        actor_loss, critic_loss = compute_losses(state_tensor, action, reward_tensor, next_state_tensor, done_tensor)\n",
    "\n",
    "        # Optimize actor\n",
    "        policy_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        policy_optimizer.step()\n",
    "\n",
    "        # Optimize critic\n",
    "        value_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        value_optimizer.step()\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "    print(f\"\\tFinished with reward {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3cbfd69-7657-4544-81f9-ce543e30feb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward = 286.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "state, _ = env.reset()\n",
    "total_reward = 0\n",
    "done = False\n",
    "    \n",
    "while not done:\n",
    "\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "    \n",
    "    env.render()  # Render the environment (optional)\n",
    "\n",
    "    # Select an action using the policy network\n",
    "    probs = policy_net(state_tensor)\n",
    "    dist = torch.distributions.Categorical(probs)\n",
    "    action = dist.sample().item()\n",
    "\n",
    "    # Step the environment\n",
    "    next_state, reward, done, trunc, _ = env.step(action)\n",
    "\n",
    "    # Update the current state\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "\n",
    "print(f\"Total Reward = {total_reward}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f445a3-b68d-4180-a214-e44dbb06ee33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68058bf-e6ab-4063-9816-512ba03172c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91fe477-855c-49bb-a24f-bac1614431cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
